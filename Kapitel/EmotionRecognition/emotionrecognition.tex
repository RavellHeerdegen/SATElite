\subsection{Gefühlserkennung}
Emotion recognition (ER) ist ein interdisziplinäres Forschungsgebiet, welches u.a.~die Bereiche Informatik und Psychologie vereint. Das Erforschen von emotionalen Zuständen ist hilfreich, um menschliche Aktionen zu verstehen, oder um menschliche Faktoren in künstliche Systeme zu integrieren \cite{eyetrackemotionrec}.
Generell werden Gesichtsausdrücke über die Muskelbewegungen im menschlichen Gesicht geregelt. Die Muskelbewegungen im Gesicht werden auch action units (AUs) genannt. Nachdem ein Gesicht erfasst und isoliert wurde, kann ein System dem Gesicht eine Emotion zuordnen mithilfe von action units analysis \cite{wildemotionrec}.
Beim Arbeiten im Bereich ER und facial expression recognition, unterscheidet man zwischen sechs generellen, kulturunabhängigen Emotionen, welche 1971 von Ekman und Friesen als universell festgelegt wurden: Wut, Ekel, Angst, Freude, Trauer und Überraschung \cite{cnnemotionrec}.
ER findet aber auch Anwendung in Verbindung mit Sprache, bzw. Körpersprache. Mit ER soll die natürliche Kommunikation zwischen Mensch und Maschine hergestellt und verbessert werden. Jedoch auch Zustände eines Menschen erkannt werden, um anschließend passende Werbung oder zugeschnittene Angebote zu erstellen.\\Methodisch wird zwischen statischen und dynamischen Verfahren unterschieden. Das bedeutet, einerseits mit statischen Bildern zu arbeiten, andererseits mit Sequenzen von Bildern, u.a.~auch Videos \cite{facialemotionrecusingcnn}.
\\
\subsubsection*{Gefühlserkennung und convolutional neural networks}
In \cite{facialemotionrecusingcnn} wird ein Versuch mit einem CNN in Verbindung mit automatic facial expression recognition (AFER) beschrieben. Bei dem Versuch wurde das Facial Action Coding System (FACS) verwendet, um die Genauigkeit der Erkennung von Emotionen zu verbessern. Das geschieht aufgrund der Möglichkeit des FACS, die AUs im Gesicht zu erkennen und zu interpretieren. Auch kann mit dem FACS die Intensität einer Emotion definiert werden. Als Trainingsdaten fungierte der Cohn-Kanade (CK+) Datensatz. Der CK+ besteht aus 123 Individuen, welche verschiedene Folgen von emotionalen Ausdrücken aufgenommen haben. Die daraus entstandenen Bilder wurden auf eine einheitliche Größe skaliert und verpixelt. Zusätzlich kamen Techniken wie z.B. Rotations- oder Farbkorrekturen zum Einsatz, bevor die Daten in das CNN gegeben wurden. Im ersten convolutional layer des eingesetzten CNN wurden visuelle Merkmale wie Kanten, Lippenformen, Falten, Augen und Augenbrauen extrahiert. Durch die berechneten shared weights zwischen den Schichten im CNN, kann ein hohes oder ein geringes Vorkommen von AUs im output layer festgestellt werden, was auf verschiedene Klassifizierungen von Emotionen schließen lässt. Ergebnis des Versuches war, dass das trainierte CNN Parallelen zwischen den Emotionen Wut und Neutralität gefunden hat, was mit den ähnlichen Gesichtszügen von Wut und Neutralität zusammenhängt. Auch hat sich ergeben, dass Freude am besten erkannt wurde. Dadurch wurde bestätigt, dass sich CNNs für die Erkennung von Emotionen in einem menschlichen Gesicht eignen.
\\
\\
Das von L. Surace et al.~beschriebene Experiment \cite{wildemotionrec} benutzt ebenfalls ein CNN, jedoch mit anderen Techniken. Zum Einsatz kommen zwei Methoden, welche sich für das Erfassen von sogenannten lokalen sowie globalen Informationen eignen. Die bottom-up Methodik eignet sich zum Isolieren von Gesichtern aus Bildern, um diese anschließend in das bereits trainierte CNN als Input einzugeben. Die top-down Methode bezeichnet sich als labeling algorithm. Beim labeling werden Bezeichnungen einer Szene zugeordnet, um den Kontext der Szene zu erfassen. Die Bezeichner werden daraufhin in ein bayesian network (BN) \cite{wildemotionrec} gegeben, welches die Bezeichner verarbeitet, um Beziehungen und Abhängigkeiten zwischen den Begriffen herzustellen. Die verarbeiteten Bezeichner werden als Input in das CNN geleitet. Der output layer des CNN besteht dadurch aus drei Ausgängen, welche die Szene als negativ, positiv oder neutral einordnen. Das bedeutet, dass eine Szene z.B. einen traurigen Kontext haben kann, wodurch eine Vorkategorisierung einer Szene durchgeführt werden kann. Dadurch kann Zeit eingespart werden, da z.B. beim Einordnen einer Szene als Traurig, eine gewisse Anzahl an Emotionen, welche positiven, fröhlichen Bezeichnern zugeordnet sind, von vornherein wegfallen. Zum Einsatz kommt außerdem die GAF Datenbank, welche eine Abbildung von 6470 Bildern von Personen in unterschiedlichen Szenen darstellt. Diese Menge wurde aufgeteilt auf die Trainings-, Validierungs- und auf die Testdaten. Zusätzlich wurde ein Dropout eingesetzt, um die Überanpassung der Neuronen zu vermeiden. Der Dropout sorgt dafür, dass teilweise shared weights übergangen bzw. gewollt verloren gehen. Letztendlich hat sich ergeben, dass das CNN in Kombination mit einem BN die besten Ergebnisse liefert, im Gegensatz zum Versuch ohne BN bzw. ohne CNN. Ein interessanter Ausbau des Versuches wird am Ende des Ergebnisses gegeben. Dabei wird erwähnt, dass die Klassifizierungen negativ, positiv und neutral, auf drei CNNs aufgeteilt werden könnten, um jedes zu spezialisieren und somit noch bessere Ergebnisse erzielen zu können.

