\subsection{Gefühlserkennung}
Emotion recognition ist ein interdisziplinäres Forschungsgebiet, welches u.a. die Bereiche Computer Sciences, Cognitive Science sowie Psychology vereint. Das Erforschen von emotionalen Zuständen ist hilfreich, um menschliche Aktionen zu verstehen, als auch um menschliche Faktoren in künstliche Systeme zu integrieren.\cite{eyetrackemotionrec}
Generell wird das Erfassen von Emotionen über die Muskelbewegungen im menschlichen Gesicht geregelt. Die Muskelbewegungen im Gesicht werden auch action units (AUs) genannt. Nachdem ein Gesicht erfasst und isoliert wurde, kann man dem Gesicht eine Emotion zuordnen mithilfe von action units analysis.\cite{wildemotionrec}
Beim Arbeiten im Bereich Emotion recognition und facial expression recognition, unterscheidet man zwischen sechs generellen, kulturunabhängigen Emotionen, welche 1971 von Ekman und Friesen als universell festgelegt wurden: Wut, Ekel, Angst, Freude, Trauer und Überraschung.\cite{cnnemotionrec}
Emotion recognition findet aber auch Anwendung in Verbindung mit Sprache, bzw. Körpersprache. Mit Emotion recognition möchte man natürliche Kommunikation zwischen Mensch und Maschine herstellen und verbessern, jedoch auch Zustände eines Menschen erkennen, um anschließend passende Werbung oder zugeschnittene Angebote zu erstellen. Methodisch wird zwischen static approaches und dynamic approaches unterschieden. Das bedeutet, einerseits mit statischen Bildern zu arbeiten, andererseits mit Sequenzen von Bildern, u.a. auch Videos. Emotion expression recognition wird grundsätzlich in die Schritte face image acquisition, feature extraction und facial emotion expression recognition unterteilt. \cite{facialemotionrecusingcnn}
\\
\subsubsection*{Gefühlserkennung und convolutional neural networks}
In \cite{facialemotionrecusingcnn} wird ein Versuch mit einem CNN in Verbindung mit automatic facial expression recognition (AFER) beschrieben. Bei dem Versuch wurde das Facial Action Coding System (FACS) verwendet, um die Genauigkeit der Erkennung von Emotionen zu verbessern. Das geschieht aufgrund der Möglichkeit des FACS, die AUs im Gesicht zu erkennen und zu interpretieren. Auch kann mit dem FACS die Intensität einer Emotion definiert werden. Als Trainingsdaten fungierte der Cohn-Kanade (CK+) Datensatz. Der CK+ besteht aus 123 Individuen, welche verschiedene Folgen von emotionalen Ausdrücken aufgenommen haben. Die daraus entstandenen Bilder wurden auf eine einheitliche Größe skaliert und verpixelt. Zusätzlich kamen pre-processing techniques wie z.B. Rotations- oder Farbkorrektur zum Einsatz, bevor die Daten in das CNN gegeben wurden. Im ersten convolutional layer des eingesetzten CNN wurden visuelle Merkmale wie Kanten, Lippenformen, Falten, Augen und Augenbrauen extrahiert. Durch die berechneten shared weights zwischen den Schichten im CNN, kann ein hohes oder ein geringes Vorkommen von AUs im output layer festgestellt werden, was auf verschiedene Klassifizierungen von Emotionen schließen lässt. Ergebnis des Versuches war, dass das trainierte CNN Parallelen zwischen den Emotionen Wut und Neutralität gefunden hat, was mit den ähnlichen Gesichtszügen von Wut und Neutralität zusammenhängt. Auch hat sich ergeben, dass Freude am besten erkannt wurde. Dadurch wurde bestätigt, dass sich CNNs für die Erkennung von Emotionen in einem menschlichen Gesicht eignen.
\\
\\
Das Experiment \cite{wildemotionrec} benutzt ebenfalls ein CNN, jedoch mit anderen Techniken. Zum Einsatz kommen zwei Methoden, welche sich für das Erfassen von sogenannten lokalen sowie globalen Informationen eignen. Die eine, genannt bottom-up, eignet sich zum Isolieren von Gesichtern aus Bildern, um diese anschließend in das bereits trainierte CNN als Input einzugeben. Die andere nennt sich top-down und bezeichnet sich als labeling algorithm. Beim labeling werden label, auch descriptors genannt, einer Szene zugeordnet, um den Kontext der Szene zu erfassen. Die descriptors werden daraufhin in ein bayesian network (BN) \cite{wildemotionrec} gegeben, welches die descriptors verarbeitet, um Beziehungen und Abhängigkeiten zwischen den Begriffen herzustellen. Die verarbeiteten descriptors werden als Input in das CNN geleitet. Der output layer des CNN besteht dadurch aus drei Ausgängen, welche die Szene als negativ, positiv oder neutral einordnen. Zum Einsatz kommt außerdem die GAF Datenbank, welche 6470 Bilder beinhaltet. Diese Menge wurde aufgeteilt auf die Traningsdaten, Validierungsdaten und auf die Testdaten. Zusätzlich wurde ein Dropout eingesetzt, um die Überanpassung der Neuronen zu vermeiden. Endlich hat sich ergeben, dass das CNN in Kombination mit einem BN die besten Ergebnisse liefert, im Gegensatz zum Versuch ohne BN bzw. ohne CNN. Ein interessanter Ausbau des Versuches wird am Ende gegeben. Dabei wird erwähnt, dass die Klassifizierungen negativ, positiv und neutral, auf drei CNNs aufgeteilt werden könnten, um jedes zu spezialisieren und somit noch bessere Ergebnisse erzielen zu können.

