\section[Kosten- und Aufwandsschätzung (Robin Schramm)]{\large  Kosten- und Aufwandsschätzung (Robin Schramm)} \label{KostenAufwand}
In den neunzehnhundertsechziger und siebziger Jahren stieg die Größe und Komplexität von Softwareprojekten drastisch an \cite{Bajta2018}. Softwareentwicklung wurde immer mehr kommerziell genutzt wodurch sich neue Arten von Kunden bildeten. Durch die Vergrößerung der Branche war dies immer weniger effektiv ohne vorherige Planung zu entwickeln. Als Lösung für das Problem entstand das Software Projekt Management (SPM), welches auch heutzutage noch eine zentrale Rolle in der Softwareentwicklung spielt \cite{Bajta2018}. Einer der Punkte mit dem sich SPM beschäftigt ist das project cost management (PCM). Nach Bajita \cite{Bajta2018} unterteilt sich PCM in die Kategorien software cost estimation (SCE) und software effort estimation (SEE). Aktuelle Forschungen zeigen, dass genaue SCE und SEE die Chance auf ein qualitativ hochwertiges Produkt erhöht \cite{Matson1994}\cite{Bilgaiyan2016}. Jeffery et al.~beschreiben, dass SCE eines der wichtigen Werkzeuge ist, um einen kompetitiven Vorteil im Bereich des IT-Service zu erhalten \cite{Jeffery1990}. Aus dieser Notwendigkeit entstanden viele SCE und SEE Tools, welche Experten bei der Projektplanung unterstützen.
\\
Im Zuge dieses Kapitels werden als Erstes die bestehenden SCE und SEE Techniken vorgestellt und kategorisiert. Als nächstes wird das constructive cost estimation model (COCOMO) sowie neuere Soft-Computing Modelle vorgestellt und deren Vorteile und Probleme aufgelistet. In den weiteren Absätzen wird erklärt, wie die Ergebnisse der Schätzungen gemessen und in Zahlen  gefasst werden können, um danach auf einige Beispiele von direkten Vergleichen der mathematischen und Soft-Computing Lösungen einzugehen. Zum Schluss wird anhand der vorherigen Abschnitte beantwortet, ob NNs in der SWT im Bereich der SCE und SEE eingesetzt werden können und wo deren derzeit bekannte Grenzen liegen.

\subsection{Stand der Technik}
In der Planungsphase eines Softwareprojekts ist es essentiell, die Kosten und die Dauer des Projekts schätzen zu können. Kunden geben Aufträge und müssen wissen, wie viel welche Funktion kosten wird und wann das Projekt voraussichtlich abgeschlossen sein wird. Grobe Schätzungen können allein durch Erfahrung der Entwickler abgegeben werden, wie zum Beispiel in SCRUM, bei dem User Stories je nach Komplexität Story Points gegeben werden. Bei ansteigender Komplexität der Projekte kann die manuelle Schätzung nicht ausreichen. Das zeigt sich im großen Zweig des SPM, welcher sich allein mit der Planung und Schätzung von Softwareprojekten auseinandersetzt. Eine wichtige Metrik in der SPM ist die Größe der Software. Dabei kann man zwei Arten von Methoden der Größenbestimmung unterscheiden. Zum Einen die geschätzten nötigen Lines of Code (LOC) und zum anderen die geschätzte Anzahl von function points (FPs). FPs sind ein Messkriterium für die Komplexität von Funktionen. Relevant sind dabei z.B. die Anzahl der Eingaben und Ausgaben sowie die wiedervewendbarkeit der Funktion. Anhand der Arten der Modellkonstruktion lassen sich SCE und SEE Modelle in vier Kategorien unterteilen. (i) die Expertensysteme, die sich auf die Erfahrung der Personen verlassen\cite{Heemstra1992}. (ii) die lineare Systeme, die auf der Größenmessung mit FP basieren\cite{Matson1994}. (iii) die nonlinearen Mathematischen Modelle. (iv) die neueren Soft Computing Ansätze, die entweder bisherige Methoden erweitern oder komplett neu aufbauen\cite{Huang2007}\cite{Huang2006}. Eines weit verbreitetes Werkzeug ist COCOMO, das sich sich den nichtlinearen Systemen zuordnen lässt \cite{Jain2016}.
\vspace{4.0cm}
\subsection{COCOMO}
 COCOMO wurde 1981 von Boehm entworfen, um Fehler im SPM zu verringern. COCOMO ist ein algorithmisches Modell mit einer Anzahl von Eingabeparametern abhängig von der Implementation. In COCOMO gibt es drei Typen von Projekten, gemessen an der Komplexität. Organic Mode für einfache kleine Projekte, embedded mode für innovative komplexe Projekte mit hohen Anforderungen sowie den semi-detached mode für Projekte zwischen dem organic und embedded type. Der Projekttyp wirkt sich neben den LOC signifikant auf die Endschätzung aus. Zudem nutzt COCOMO einige Kostentreiber (cost drivers), welche je nach Art variieren. Beispiele dafür sind die Erfahrung der Programmierer in den verwendeten Technologieren oder die geforderte prozentuale Wiederverwendbarkeit der Software. In Boehms ersten COCOMO gab es 15 cost drivers. Cost drivers werden mit Werten zwischen \textit{sehr niedrig} und \textit{sehr hoch} bewertet. Die Werte der cost drivers stellen dabei Gewichtungen dar, die sich ebenfalls auf den Endwert auswirken. Das Ergebnis stellt die Schätzung der Kosten in einer gewünschten Währung dar, sowie die voraussichtlich benötigten Personenmonate. Klassischen Modelle wie COCOMO bringen Probleme mit sich, von denen hier einige beschrieben werden.
 \\
Algorithmische Tools sind generisch und somit nicht an das spezifische Projekt angepasst. Fehlende Anpassung an ein Projekt kann bei generischen algorithmischen Tools sehr ungenaue Ergebnisse liefern. Um ein generisches algorithmisches Tool anzupassen, ist eine große Menge an Daten notwendig, welche oft nicht vorhanden ist. Daten von anderen Firmen sind in der Regel nicht verfügbar. Erfahrung in großen Projekten ist schwierig, da diese eine lange Lebensdauer haben, bis sie abgeschlossen werden und die Schätzung mit dem Ergebnis verglichen werden kann. Oft werden die Schätzungen nebenbei gemacht und nicht als eigenständiger Punkt in der Planung beachtet, was die Wahl der Eingabeparameter negativ beeinflusst. Die Eingabeparameter beeinflussen die Schätzung jedoch maßgeblich. Mangel an Flexibilität bei sich ändernden Anforderungen. Dies wirkt sich besonders im agilen Bereich aus. Schwierigkeit und Aufwand, Projekt-spezifische Daten zu sammeln, zu beurteilen und in konkreten Zahlen auszudrücken. Einige der genannten Probleme versuchen verschiedene Forscher mit Soft-Computing Ansätzen zu lösen \cite{Bilgaiyan2016}\cite{Chen2005}\cite{Heemstra1992}\cite{Abrahamsson2007}.
\vspace{6.0cm}
\subsection{Soft-Computing Ansätze}
Unter Soft-Computing versteht man vor allem Fuzzy Logic, Evolutionäre Algorithmen und Künstliche Neuronale Netze (NNs). Mit ihnen kann das Problem gelöst werden, für jedes Projekt ein neues Schätzungsmodell erstellen zu müssen, da sich Soft-Computing Modelle leicht generalisieren lassen. Zudem sind sind sehr flexibel und anpassungsfähig, was sie bei sich ändernden Anforderungen wertvoll macht~\cite{Boetticher2001}. Damit sie zufriedenstellende Ergebnisse liefern, müssen die Trainingsdaten passend gewählt sein. NNs mit zufällig gewählten Trainingsdaten liefern keine besseren Schätzungen als klassische Modelle. \cite{Setyawati2002}
\\
\subsection{Leistungsbeurteilung}
Um die Modelle untereinander vergleichen zu können, gibt es verschiedene Methoden die Qualität zu messen. Nach Finnie et al.~\cite{Finnie1996}~ist der mean absolute relative error (MARE) die bevorzugte Fehlerbestimmungsmethode von Software-Messforschern. Der MARE wird wie folgt berechnet:
\begin{equation}
\left|\frac{estimate - actual}{estimate}\right|
\end{equation}
\textit{estimate} ist dabei der vom Modell geschätzte Wert und \textit{actual} der nach der Durchführung tatsächlich gemessene Wert. Um herauszufinden, ob das Modell über- oder unterschätzt, wird der mean relative error (MRE) wie folgt berechnet:
\begin{equation}
\left(\frac{estimate - actual}{estimate}\right)*100
\end{equation}
Ein Wert von Null ist dabei ein perfektes Ergebnis. Je höher oder niedriger die Zahl, desto größer wurde über- oder unterschätzt \cite{Finnie1996}. Ein Ergebnis zwischen -25 und 25 gilt als gutes Ergebnis in SEE \cite{Abrahamsson2007}. Viele der vorgestellten Modelle werden an laufenden Industrieprojekten getestet. Das am weitesten akzeptierte Evaluationskriterium ist der MRE aus einem Satz von 63 NASA Projekte \cite{Khalifelu2012}.
\vspace{3.0cm}
\subsection{Vergleich}
Abrahamsson et al.~vergleichen in ihrem Artikel inkrementelle Ansätze mit globalen Ansätzen im Kontext der agilen Entwicklung. Die verwendeten Modelle sind einerseits die Schrittweise lineare Regression und anderseits zwei verschiedene Arten von NNs. Eines ihrer Ergebnisse war, dass NNs nicht generell besser sind als einfache Regression. Das Multilayer Perceptron (MLP) aus dem Test liefert zum Beispiel schlechtere Ergebnisse als die meisten Regressionsmodelle. Das Radial Basis Function (RBF) NN lieferte ähnliche Ergebnisse wie das Regressionsmodell~\cite{Abrahamsson2007}.
Khalifelu et al.~vergleichen in ihrem Artikel COCOMO und vier verschiedene Soft-Computing Modelle. Die Trainingsdaten stammen von NASA Projekten. Das KNN und die Support Vector Machine, eine Art des Maschinellen Lernens, lieferten genauere Schätzungen als alle anderen getesteten Ansätze. Zusätzlich waren die Soft-Computing Modelle effizienter und somit schneller als COCOMO \cite{Khalifelu2012}.
Gharehchopogh testet in seinem Artikel ein NN Modell und COCOMO an 11 NASA Projekten und vergleicht deren Ergebnisse für die Kostenschätzung. Das NN hat in 90\% der getesteten Projekte einen niedrigeren MRE erzielt als COCOMO. In zehn der elf Projekte betrug der MRE des NNs weniger als 20\%, was in der SCE als gutes Ergebnis angesehen wird \cite{Gharehchopogh2011}\cite{Abrahamsson2007}.