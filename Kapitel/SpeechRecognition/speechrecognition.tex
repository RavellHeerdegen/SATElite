\subsection{Spracherkennung}
Speech recognition (SR) macht es Maschinen möglich, Sprache in Text oder Kommandos zu übersetzen. Dies geschieht sowohl durch Identifikation als auch durch Verstehen der eingegebenen Sprachsignale. Die Sprachsignale werden auf erlernte Muster angewendet, wobei Merkmale entnommen werden, um die Signale zuordnen zu können. Einerseits ist dadurch die Kommunikation zwischen Mensch und Maschine möglich, andererseits soll dadurch die menschliche Sprache in Kommandos für Maschinen übersetzt werden können~\cite{technology}.\\
\\
\subsection*{Hidden Markov Model}
Das Hidden Markov Model (HMM) ist bekannt für seinen Nutzen im Be\-reich der speech recognition~\cite{residualnn}. HMM ist in der Lage Wörter zu modellieren, welche daraufhin einem bestimmten Kontext zugeordnet werden können. Der HMM-Algorithmus gehört zu den unsupervised learning techniques, da er Parameter unkontrolliert schätzt ohne zusätzliche Kennzeichnungen durch Menschen~\cite{hmm}. Ein MLP-NN weißt während des Trainings jedem Neuron eine Gewichtung zu, um den Einfluss zwischen Input und Output zu ma\-ximieren~\cite{Khalifelu2012}. Radial Basis Function-NNs (RBF) werden in den Gebieten der pattern recognition und Funktionsapproximation eingesetzt. 

\subsubsection{Spracherkennung und convolutional neural networks}
In~\cite{usingcnn} beschreibt Du Guiming et al.~einen Versuch, bei dem SR in Verbindung mit einem CNN getestet wird. Die dafür verwendeten Trainingsdaten stammen von 30 Personen. Für die automatische Erkennung der Sprachsignale und ihrer Tonhöhen wird der Mel Frequency Cepstral Coefficient (MFCC)~\cite{MFCC} eingesetzt, welcher das Frequenzspektrum kompakt darstellen kann. Nach der Verarbeitung der Signale, wird das neuronale Netz mit fünf Personen getestet. Beim ersten Durchlauf der Signale durch das neuronale Netz, sind die erwarteten und vorhergesehenen Werte alle gleich. Der Unterschied zwi\-schen erwarteten und echten Werten wird als cost function bezeichnet. Ist die cost function niedrig, sind die Parameter hochwertiger. Es zeigt sich, dass mit steigender Durchlaufzahl die cost function niedriger wird. Somit kann bestätigt werden, dass sich CNNs sowohl für die Erkennung von isolierten Wörtern eignen, als auch für die Reduzierung von Varianzen~\cite{usingcnn}.\\
\\
Rafael M.~Santos et al.~zeigen in~\cite{noisycnn} auf, wie sich ein CNN in einer geräuschvol\-len Umgebung verhält. Da sich die sogenannten shared weights in einem CNN über den gesamten Eingaberaum erstrecken, können Merkmale selbst dann gefunden werden, wenn sie sich aufgrund von Störungen z.B. durch Lautstärke an unterschiedlichen Positionen befinden. In dem Experiment wird das CNN so trainiert, dass es die HMM-Verteilungswahrscheinlich\-keit schätzen kann. Außerdem klassifiziert das CNN nicht nur die aktuelle Aufnahme, sondern bezieht vorherige und nachkommende Aufnahmen mit ein, um Zusammenhän\-ge zwischen den Informationen herstellen zu können. Als Eingabe werden einzel\-ne Wörter mit ansteigender Lautstärke eingesetzt.
\\
\\
\\
\\
Die Eingabedaten bestehen aus brasilianischen und portugiesischen Wörtern, welche zehn mal von jeweils sechs Männern und zwei Frauen vorgetragen werden. Die Aufnahmen werden in einer unkontrollierten Umgebung mit einem Mobiltelefon aufgezeichnet. Bei dem Versuch hat sich ergeben, dass die Kombination aus einem CNN und HMM, sowohl Merkmale als auch Signale erfolgreich filtern und erkennen kann. Die Studie zeigt, dass sich die Kombination CNN-HMM am wenigsten von Lautstärke und Geräuschen beeinflussen lässt, im Vergleich zu anderen getesteten Kombinationen wie GMM~\cite{svmgmm} oder SVM~\cite{svmgmm} in Verbindung mit HMM.

\subsubsection{Spracherkennung und residual neural networks}
Im Versuch von H. Vydana und A. Vuppala~\cite{residualnn} wird ein Resnet mit mehreren Res in Kombination mit SR getestet. Es hat sich zunächst herausgestellt, dass mit einer Anzahl von acht Res, welche hintereinander geschaltet werden, die beste Performanz in Bezug auf die word error rate erreicht wird. Ab einem Wert von mehr als acht Res, nimmt die Performanz ab. Das bedeutet, dass es eine Grenze gibt, welche sowohl die Performanz, aber auch die Anwendbarkeit eines Resnet eingrenzt. Verglichen wird das sogenannte 8Res dabei mit einem 8-DNN. Dabei hat sich herausgestellt, dass das 8Res bessere Ergebnisse erzielt als das 8-DNN. Schließlich wurde festgestellt, dass HMM-Resnets mit steigender Anzahl an Res und Layern eine um 0,4\% bessere Performanz als ein HMM-DNN~\cite{residualnn} wie z.B.~CNN mit HMM in Bezug auf die word error rate erreichten. Somit können Resnets DNNs in Bezug auf SR vorgezogen werden, wenn die Anzahl der Res und DNN-Layer beachtet wird.


