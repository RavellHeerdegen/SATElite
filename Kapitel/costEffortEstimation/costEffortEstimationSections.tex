\section{Kosten- und Aufwandsschätzung (Robin Schramm)} \label{KostenAufwand}
In den sechziger und siebziger Jahren stieg die Größe und Komplexität von Softwareprojekten drastisch an\cite{Bajta2018}. Softwareentwicklung wurde immer mehr kommerziell genutzt wodurch sich neue Arten von Kunden bildeten. Darunter vor allem das Militär. Durch die Vergrößerung der Branche war dies immer weniger effektiv ohne vorherige Planung zu entwickeln. Als Lösung für das Problem entstand das Software Projekt Management (SPM), welches auch heutzutage noch eine zentrale Rolle in der Softwareentwicklung spielt\cite{Bajta2018}. Einer der Punkte mit dem sich SPM beschäftigt ist das project cost management(PCM). Nach Bajita\cite{Bajta2018} unterteilt sich PCM in die Kategorien software cost estimation (SCE) und software effort estimation (SEE). Aktuelle Forschungen zeigen, dass genaue SCE und SEE die Chance auf ein qualitativ hochwertiges Produkt erhöht \cite{Matson1994}\cite{Bilgaiyan2016}. Jeffery et al.~beschreiben, dass SCE eines der wichtigen Werkzeuge ist, um einen kompetitiven Vorteil im Bereich des IT-Service zu erhalten\cite{Jeffery1990}. Aus dieser Notwendigkeit entstanden viele SCE und SEE Tools, welche Experten bei der Projektplanung unterstützen.
\\
Im Zuge dieses Kapitels werden als Erstes die bestehenden SCE und SEE Techniken vorgestellt und kategorisiert. Als nächstes wird das constructive cost estimation model (COCOMO) sowie neuere Soft-Computing Modelle vorgestellt und deren Vorteile und Probleme aufgelistet. In den weiteren Absätzen wird erklärt, wie die Ergebnisse der Schätzungen gemessen und in Zahlen  gefasst werden können, um danach auf einige Beispiele von direkten Vergleichen der mathematischen und Soft-Computing Lösungen einzugehen. Zum Schluss wird anhand der vorherigen Abschnitte beantwortet, ob NNs in der SWT im Bereich der SCE und SEE eingesetzt werden können und wo deren derzeit bekannte Grenzen liegen.

\subsection{Stand der Technik}
In der Planungsphase eines Softwareprojekts ist es essentiell, die Kosten und die Dauer des Projekts geschätzt zu haben. Kunden geben Aufträge und müssen wissen, wie viel welche Funktion kosten würde und wann das Projekt voraussichtlich abgeschlossen sein wird. Grobe Schätzungen können allein durch Erfahrung der Entwickler abgegeben werden, wie zum Beispiel in SCRUM, bei dem User Stories je nach Komplexität Story Points gegeben werden. Bei ansteigender Komplexität der Projekte kann die manuelle Schätzung nicht ausreichen. Das zeigt sich im großen Zweig des SPM, welcher sich allein mit der Planung und Schätzung von Softwareprojekten auseinandersetzt. Eine wichtige Metrik in der SPM ist die Größe der Software. Dabei kann man zwei Arten von Methoden der Größenbestimmung unterscheiden. Zum Einen die geschätzten nötigen Lines of Code (LOC) und zum anderen die geschätzte Anzahl von function points (FPs). Anhand der Arten der Modellkonstruktion lassen sich SCE und SEE Modelle in vier Kategorien unterteilen. Erstens Expertensysteme, die sich auf die Erfahrung der Personen verlassen\cite{Heemstra1992}. Zweitens die lineare Systeme, die auf der Größenmessung mit FP basieren\cite{Matson1994}. Drittens die nonlinearen Mathematischen Modelle. Viertens die neueren Soft Computing Ansätze, die entweder bisherige Methoden erweitern oder komplett neu aufbauen\cite{Huang2007}\cite{Huang2006}. Eines der beliebtesten Tools ist COCOMO.\cite{Jain2016} COCOMO lässt sich den nichtlinearen Systemen zuordnen.

\subsection{COCOMO}
 COCOMO wurde 1981 von Boehm entworfen, um Fehler im SPM zu verringern. COCOMO ist ein algorithmisches Modell mit einer Anzahl von Eingabeparametern abhängig von der Implementation. In COCOMO gibt es drei Typen von Projekten, gemessen an der Komplexität. Organic Mode für einfache kleine Projekte, embedded mode für innovative komplexe Projekte mit hohen Anforderungen sowie den semi-detached mode für Projekte zwischen dem organic und embedded type. Der Projekttyp wirkt sich neben den LOC signifikant auf die Endschätzung aus. Zudem nutzt COCOMO einige Kostentreiber (cost drivers), welche je nach Art variieren. In Boehms ersten COCOMO gab es 15 cost drivers. Cost drivers werden mit Werten zwischen \textit{sehr niedrig} und \textit{sehr hoch} bewertet. Die Werte stellen dabei Gewichtungen dar, die sich ebenfalls auf den Endwert auswirken. Das Ergebnis stellt die Schätzung der Kosten in einer gewünschten Währung dar, sowie die benötigten Personenmonate. Die klassischen Modelle bringen Probleme mit sich, von denen hier einige beschrieben werden.
 \\
Algorithmische Tools sind generisch und somit nicht an das spezifische Projekt angepasst, daraus ergibt sich ein Mangel an bestehenden Daten für die eigene Nische. Daten von anderen Firmen sind in der Regel nicht verfügbar. Erfahrung in großen Projekten ist schwierig, da diese eine lange Lebensdauer haben, bis sie abgeschlossen werden und die Schätzung mit dem Ergebnis verglichen werden kann. Oft werden die Schätzungen nebenbei gemacht und nicht als eigenständiger Punkt in der Planung beachtet, was die Wahl der Eingabeparameter negativ beeinflusst. Die Eingabeparameter beeinflussen die Schätzung jedoch maßgeblich. Mangel an Flexibilität bei sich ändernden Anforderungen. Dies wirkt sich besonders im Agilen Bereich aus. Schwierigkeit und Aufwand, Projekt spezifische Daten zu sammeln, zu beurteilen und in konkreten Zahlen auszudrücken. Einige der genannten Probleme versuchen verschiedene Forscher mit Soft-Computing Ansätzen zu lösen\cite{Bilgaiyan2016}\cite{Chen2005}\cite{Heemstra1992}\cite{Abrahamsson2007}.

\subsection{Soft-Computing Ansätze}
Unter Soft-Computing versteht man vor allem Fuzzy Logic, Evolutionäre Algorithmen und Künstliche Neuronale Netze (NNs). Mit ihnen kann das Problem gelöst werden, für jedes Projekt ein neues Schätzungsmodell erstellen zu müssen, da sie sich leicht generalisieren lassen. Zudem sind sind sehr flexibel und anpassungsfähig, was sie bei sich ändernden Anforderungen wertvoll macht.\cite{Boetticher2001} Damit sie zufriedenstellende Ergebnisse liefern, müssen die Trainingsdaten passend gewählt sein. NNs mit zufällig gewählten Trainingsdaten liefern keine besseren Schätzungen als Regressionsmodelle. \cite{Setyawati2002}

\subsection{Leistungsbeurteilung}
Um die Modelle untereinander vergleichen zu können, gibt es verschiedene Methoden die Qualität zu messen. Nach Finnie et al\cite{Finnie1996}.~ist der mean absolute relative error (MARE) die bevorzugte Fehlerbestimmungsmethode von Software-Messforschern. Der MARE wird wie folgt berechnet:
\begin{equation}
\left|\frac{estimate - actual}{estimate}\right|
\end{equation}
\textit{estimate} ist dabei der vom Modell geschätzte Wert und \textit{actual} der nach der Durchführung tatsächlich gemessene Wert. Um herauszufinden, ob das Modell über- oder unterschätzt, wird der mean relative error (MRE) wie folgt berechnet:
\begin{equation}
\left(\frac{estimate - actual}{estimate}\right)*100
\end{equation}
Ein Wert von Null ist dabei ein perfektes Ergebnis. Je höher oder niedriger die Zahl, desto größer wurde über- oder unterschätzt.\cite{Finnie1996} Ein Ergebnis zwischen -25 und 25 gilt als gutes Ergebnis in SEE.\cite{Abrahamsson2007}. Viele der vorgestellten Modelle werden an laufenden Industrieprojekten getestet. Das am weitesten akzeptierte Evaluationskriterium ist der MRE aus einem Satz von 63 NASA Projekten.\cite{Khalifelu2012}

\subsection{Vergleich}
Abrahamsson et. al. vergleichen in ihrem Artikel inkrementelle Ansätze mit globalen Ansätzen im Kontext der agilen Entwicklung. Die verwendeten Modelle sind einerseits die Schrittweise lineare Regression und anderseits zwei verschiedene Arten von NNs. Eines ihrer Ergebnisse war, dass NNs nicht generell besser sind als einfache Regression. Das Multilayer Perceptron (MLP) aus dem Test liefert zum Beispiel schlechtere Ergebnisse als die meisten Regressionsmodelle. Das Radial Basis Function (RBF) NN lieferte ähnliche Ergebnisse wie das Regressionsmodell.\cite{Abrahamsson2007}
TODO - Besser als COCOMO\cite{Khalifelu2012} %TODO
TODO - Auch besser als COCOMO in NASA Testprojekten in über 90 Prozent\cite{Gharehchopogh2011} %TODO